{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from utils import load_datasets, load_target\n",
    "import models\n",
    "from models.tuning import beyesian_optimization\n",
    "from models.evaluation import cross_validation_score\n",
    "import json\n",
    "\n",
    "n_trials = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = json.load(open('./config/default.json'))\n",
    "# X_train, X_test = load_datasets([\"Age\", \"AgeSplit\", \"EducationNum\"])\n",
    "X_train, X_test = load_datasets(config['features'])\n",
    "y_train = load_target('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-02-21 17:32:33,838]\u001b[0m A new study created in memory with name: no-name-849f9969-7e1a-4079-9bde-422fe09f4207\u001b[0m\n",
      "/Users/yutahirai/opt/anaconda3/envs/tensorflow26/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/yutahirai/opt/anaconda3/envs/tensorflow26/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-02-21 17:55:40,565]\u001b[0m Trial 0 finished with value: 0.10711592466965873 and parameters: {'learning_rate': 0.055332536888805156, 'lambda_l1': 7.151893666572301, 'lambda_l2': 6.0276337646888045, 'bagging_freq': 4, 'min_child_samples': 45}. Best is trial 0 with value: 0.10711592466965873.\u001b[0m\n",
      "/Users/yutahirai/opt/anaconda3/envs/tensorflow26/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/yutahirai/opt/anaconda3/envs/tensorflow26/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-02-21 18:17:25,776]\u001b[0m Trial 1 finished with value: 0.086571182880047 and parameters: {'learning_rate': 0.06494351719359896, 'lambda_l1': 4.375872118251053, 'lambda_l2': 8.917730008903067, 'bagging_freq': 7, 'min_child_samples': 41}. Best is trial 1 with value: 0.086571182880047.\u001b[0m\n",
      "/Users/yutahirai/opt/anaconda3/envs/tensorflow26/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/yutahirai/opt/anaconda3/envs/tensorflow26/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    }
   ],
   "source": [
    "### lightgbm\n",
    "### 深さが浅い・中・深いものを作る\n",
    "\n",
    "# 浅いやつ\n",
    "lgbm = models.Lgbm({})\n",
    "optimized_params = beyesian_optimization(lgbm, X_train, y_train, {\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': 10,\n",
    "    'learning_rate': [0.001, 0.1],\n",
    "    'lambda_l1': [1e-8, 10.0],\n",
    "    'lambda_l2': [1e-8, 10.0],\n",
    "    'bagging_freq': [1, 7],\n",
    "    'min_child_samples': [5, 100],\n",
    "    'learning_rate': [0.001, 0.1],\n",
    "    'max_depth': 5,\n",
    "    'random_state': 0,\n",
    "    'num_boost_round': 10000,\n",
    "    'verbosity': -100,\n",
    "}, n_trials)\n",
    "optimized_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimized_params)\n",
    "print(cross_validation_score(models.Lgbm(optimized_params), X_train, y_train))\n",
    "with open('./config/Lgbm-depth-5.json', 'w') as f:\n",
    "    json.dump(optimized_params, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### lightgbm\n",
    "### 深さが浅い・中・深いものを作る\n",
    "\n",
    "# 浅いやつ\n",
    "lgbm = models.Lgbm(config[\"model_params\"][\"Lgbm\"])\n",
    "optimized_params = beyesian_optimization(lgbm, X_train, y_train, {\n",
    "    'objective': 'binary',\n",
    "    'learning_rate': [0.001, 0.1],\n",
    "    'lambda_l1': [1e-8, 10.0],\n",
    "    'lambda_l2': [1e-8, 10.0],\n",
    "    'bagging_freq': [1, 7],\n",
    "    'min_child_samples': [5, 100],\n",
    "    'learning_rate': [0.001, 0.1],\n",
    "    'max_depth': 15,\n",
    "    'random_state': 0,\n",
    "    'num_boost_round': 10000,\n",
    "    'verbosity': -100,\n",
    "}, n_trials)\n",
    "optimized_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimized_params)\n",
    "print(cross_validation_score(models.Lgbm(optimized_params), X_train, y_train))\n",
    "with open('./config/Lgbm-depth-15.json', 'w') as f:\n",
    "    json.dump(optimized_params, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### lightgbm\n",
    "### 深さが浅い・中・深いものを作る\n",
    "\n",
    "# 浅いやつ\n",
    "lgbm = models.Lgbm(config[\"model_params\"][\"Lgbm\"])\n",
    "optimized_params = beyesian_optimization(lgbm, X_train, y_train, {\n",
    "    'objective': 'binary',\n",
    "    'learning_rate': [0.001, 0.1],\n",
    "    'lambda_l1': [1e-8, 10.0],\n",
    "    'lambda_l2': [1e-8, 10.0],\n",
    "    'bagging_freq': [1, 7],\n",
    "    'min_child_samples': [5, 100],\n",
    "    'learning_rate': [0.001, 0.1],\n",
    "    'max_depth': -1,\n",
    "    'random_state': 0,\n",
    "    'num_boost_round': 10000,\n",
    "    'verbosity': -100,\n",
    "}, n_trials)\n",
    "optimized_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimized_params)\n",
    "print(cross_validation_score(models.Lgbm(optimized_params), X_train, y_train))\n",
    "with open('./config/Lgbm-depth-inf.json', 'w') as f:\n",
    "    json.dump(optimized_params, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = models.RandomForest({})\n",
    "optimized_params = beyesian_optimization(rf, X_train, y_train, {\n",
    "    'n_estimators': [1, 1024],\n",
    "    'max_depth': [1, 256],\n",
    "    'min_samples_split': [2, 64],\n",
    "    'min_samples_leaf': [1, 32],\n",
    "    'random_state': 0\n",
    "}, n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimized_params)\n",
    "print(cross_validation_score(models.RandomForest(optimized_params), X_train, y_train))\n",
    "with open('./config/RandomForest.json', 'w') as f:\n",
    "    json.dump(optimized_params, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etr = models.ERT({})\n",
    "optimized_params = beyesian_optimization(etr, X_train, y_train, {\n",
    "    'n_estimators': [2, 1024],\n",
    "    'max_depth': [1, 256],\n",
    "    'min_samples_split': [2, 64],\n",
    "    'min_samples_leaf': [1, 32],\n",
    "    'random_state': 0\n",
    "}, n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimized_params)\n",
    "print(cross_validation_score(models.ERT(optimized_params), X_train, y_train))\n",
    "with open('./config/ERT.json', 'w') as f:\n",
    "    json.dump(optimized_params, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn = models.NN({})\n",
    "# optimized_params = beyesian_optimization(nn, X_train, y_train, {\n",
    "#     'layers': 4,\n",
    "#     'dropout': [0.00001, 0.9],\n",
    "#     'units': 10,\n",
    "#     'nb_epoch': 100,\n",
    "# }, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(optimized_params)\n",
    "# nn = models.NN(optimized_params)\n",
    "# print(cross_validation_score(nn, X_train, y_train))\n",
    "# with open('./config/NN-shallow.json', 'w') as f:\n",
    "#     json.dump(optimized_params, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn = models.NN({})\n",
    "# optimized_params = beyesian_optimization(nn, X_train, y_train, {\n",
    "#     'layers': 10,\n",
    "#     'dropout': [0.00001, 0.9],\n",
    "#     'units': 20,\n",
    "#     'nb_epoch': 100\n",
    "# }, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(optimized_params)\n",
    "# nn = models.NN(optimized_params)\n",
    "# # nn.save_model('./config/NN-deep.h5')\n",
    "# print(cross_validation_score(nn, X_train, y_train))\n",
    "# with open('./config/NN-deep.json', 'w') as f:\n",
    "#     json.dump(optimized_params, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# とりあえず決め打ち\n",
    "optimized_params = {\n",
    "    'patience': 30,\n",
    "    'layers': 2,\n",
    "    'dropout': 0.1,\n",
    "    'units': 100,\n",
    "    'nb_epoch': 100,\n",
    "    'batch_size': 256\n",
    "}\n",
    "nn = models.NN(optimized_params)\n",
    "print(cross_validation_score(nn, X_train, y_train))\n",
    "print(optimized_params)\n",
    "with open('./config/NN-shallow.json', 'w') as f:\n",
    "    json.dump(optimized_params, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# とりあえず決め打ち\n",
    "optimized_params = {\n",
    "    'patience': 30,\n",
    "    'layers': 8,\n",
    "    'dropout': 0.1,\n",
    "    'units': 100,\n",
    "    'nb_epoch': 1000,\n",
    "    'batch_size': 256\n",
    "}\n",
    "nn = models.NN(optimized_params)\n",
    "print(cross_validation_score(nn, X_train, y_train))\n",
    "print(optimized_params)\n",
    "with open('./config/NN-deep.json', 'w') as f:\n",
    "    json.dump(optimized_params, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3f91d3689ed9f35fe62b472acb20385518ac1a4462c3f395fef40efd7e4015f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('matlab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
