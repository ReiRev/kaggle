{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from utils import load_datasets, load_target\n",
    "import models\n",
    "from models.tuning import beyesian_optimization\n",
    "from models.evaluation import cross_validation_score\n",
    "import json\n",
    "\n",
    "n_trials = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = json.load(open('./config/default.json'))\n",
    "# X_train, X_test = load_datasets([\"Age\", \"AgeSplit\", \"EducationNum\"])\n",
    "X_train, X_test = load_datasets(config['features'])\n",
    "y_train = load_target('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### lightgbm\n",
    "# ### 深さが浅い・中・深いものを作る\n",
    "\n",
    "# # 浅いやつ\n",
    "# lgbm = models.Lgbm({})\n",
    "# optimized_params = beyesian_optimization(lgbm, X_train, y_train, {\n",
    "#     'objective': 'multiclass',\n",
    "#     'num_class': 10,\n",
    "#     'learning_rate': [0.001, 0.1],\n",
    "#     'lambda_l1': [1e-8, 10.0],\n",
    "#     'lambda_l2': [1e-8, 10.0],\n",
    "#     'bagging_freq': [1, 7],\n",
    "#     'min_child_samples': [5, 100],\n",
    "#     'learning_rate': [0.001, 0.1],\n",
    "#     'max_depth': 5,\n",
    "#     'random_state': 0,\n",
    "#     'num_boost_round': 10000,\n",
    "#     'verbosity': -100,\n",
    "# }, n_trials)\n",
    "# optimized_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(optimized_params)\n",
    "# print(cross_validation_score(models.Lgbm(optimized_params), X_train, y_train))\n",
    "# with open('./config/Lgbm-depth-5.json', 'w') as f:\n",
    "#     json.dump(optimized_params, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### lightgbm\n",
    "# ### 深さが浅い・中・深いものを作る\n",
    "\n",
    "# # 浅いやつ\n",
    "# lgbm = models.Lgbm({})\n",
    "# optimized_params = beyesian_optimization(lgbm, X_train, y_train, {\n",
    "#     'objective': 'multiclass',\n",
    "#     'num_class': 10,\n",
    "#     'learning_rate': [0.001, 0.1],\n",
    "#     'lambda_l1': [1e-8, 10.0],\n",
    "#     'lambda_l2': [1e-8, 10.0],\n",
    "#     'bagging_freq': [1, 7],\n",
    "#     'min_child_samples': [5, 100],\n",
    "#     'learning_rate': [0.001, 0.1],\n",
    "#     'max_depth': 15,\n",
    "#     'random_state': 0,\n",
    "#     'num_boost_round': 10000,\n",
    "#     'verbosity': -100,\n",
    "# }, n_trials)\n",
    "# optimized_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(optimized_params)\n",
    "# print(cross_validation_score(models.Lgbm(optimized_params), X_train, y_train))\n",
    "# with open('./config/Lgbm-depth-15.json', 'w') as f:\n",
    "#     json.dump(optimized_params, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-02-23 11:30:44,440]\u001b[0m A new study created in memory with name: no-name-220f08b3-904c-4f05-92f8-cef76b29aa0f\u001b[0m\n",
      "/Users/yutahirai/opt/anaconda3/envs/tensorflow26/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/yutahirai/opt/anaconda3/envs/tensorflow26/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-02-23 11:36:12,748]\u001b[0m Trial 0 finished with value: 0.09934428983574559 and parameters: {'learning_rate': 0.055332536888805156, 'lambda_l1': 7.151893666572301, 'lambda_l2': 6.0276337646888045, 'bagging_freq': 4, 'min_child_samples': 45}. Best is trial 0 with value: 0.09934428983574559.\u001b[0m\n",
      "/Users/yutahirai/opt/anaconda3/envs/tensorflow26/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/yutahirai/opt/anaconda3/envs/tensorflow26/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-02-23 11:42:05,826]\u001b[0m Trial 1 finished with value: 0.08024948402597933 and parameters: {'learning_rate': 0.06494351719359896, 'lambda_l1': 4.375872118251053, 'lambda_l2': 8.917730008903067, 'bagging_freq': 7, 'min_child_samples': 41}. Best is trial 1 with value: 0.08024948402597933.\u001b[0m\n",
      "/Users/yutahirai/opt/anaconda3/envs/tensorflow26/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/yutahirai/opt/anaconda3/envs/tensorflow26/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-02-23 11:46:58,493]\u001b[0m Trial 2 finished with value: 0.08615348972801216 and parameters: {'learning_rate': 0.0793807787701838, 'lambda_l1': 5.2889492022400955, 'lambda_l2': 5.680445615258877, 'bagging_freq': 7, 'min_child_samples': 11}. Best is trial 1 with value: 0.08024948402597933.\u001b[0m\n",
      "/Users/yutahirai/opt/anaconda3/envs/tensorflow26/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/yutahirai/opt/anaconda3/envs/tensorflow26/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-02-23 11:56:08,284]\u001b[0m Trial 3 finished with value: 0.16752201780093035 and parameters: {'learning_rate': 0.00962580067045253, 'lambda_l1': 0.2021839842010732, 'lambda_l2': 8.32619845715318, 'bagging_freq': 6, 'min_child_samples': 88}. Best is trial 1 with value: 0.08024948402597933.\u001b[0m\n",
      "/Users/yutahirai/opt/anaconda3/envs/tensorflow26/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/yutahirai/opt/anaconda3/envs/tensorflow26/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-02-23 11:59:23,292]\u001b[0m Trial 4 finished with value: 0.10340030119064574 and parameters: {'learning_rate': 0.09788321588104364, 'lambda_l1': 7.991585644175649, 'lambda_l2': 4.6147936279145245, 'bagging_freq': 6, 'min_child_samples': 16}. Best is trial 1 with value: 0.08024948402597933.\u001b[0m\n",
      "/Users/yutahirai/opt/anaconda3/envs/tensorflow26/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/yutahirai/opt/anaconda3/envs/tensorflow26/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-02-23 12:05:33,304]\u001b[0m Trial 5 finished with value: 0.051326789555165744 and parameters: {'learning_rate': 0.06435218111142486, 'lambda_l1': 1.433532882656931, 'lambda_l2': 9.44668917104915, 'bagging_freq': 4, 'min_child_samples': 44}. Best is trial 5 with value: 0.051326789555165744.\u001b[0m\n",
      "/Users/yutahirai/opt/anaconda3/envs/tensorflow26/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/yutahirai/opt/anaconda3/envs/tensorflow26/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-02-23 12:11:27,948]\u001b[0m Trial 6 finished with value: 0.10512415755286442 and parameters: {'learning_rate': 0.027191005598358072, 'lambda_l1': 7.742336896599829, 'lambda_l2': 4.561503327603981, 'bagging_freq': 4, 'min_child_samples': 6}. Best is trial 5 with value: 0.051326789555165744.\u001b[0m\n",
      "/Users/yutahirai/opt/anaconda3/envs/tensorflow26/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/yutahirai/opt/anaconda3/envs/tensorflow26/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-02-23 12:15:12,124]\u001b[0m Trial 7 finished with value: 0.09320440616343448 and parameters: {'learning_rate': 0.062145914210511834, 'lambda_l1': 6.120957231103256, 'lambda_l2': 6.1693399725782285, 'bagging_freq': 7, 'min_child_samples': 70}. Best is trial 5 with value: 0.051326789555165744.\u001b[0m\n",
      "/Users/yutahirai/opt/anaconda3/envs/tensorflow26/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/yutahirai/opt/anaconda3/envs/tensorflow26/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-02-23 12:20:48,834]\u001b[0m Trial 8 finished with value: 0.08093793851890566 and parameters: {'learning_rate': 0.036591282156804815, 'lambda_l1': 4.370319543623094, 'lambda_l2': 6.976311962296336, 'bagging_freq': 1, 'min_child_samples': 69}. Best is trial 5 with value: 0.051326789555165744.\u001b[0m\n",
      "/Users/yutahirai/opt/anaconda3/envs/tensorflow26/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/yutahirai/opt/anaconda3/envs/tensorflow26/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-02-23 12:25:07,443]\u001b[0m Trial 9 finished with value: 0.057152628158907565 and parameters: {'learning_rate': 0.06739314909219779, 'lambda_l1': 2.103825618634583, 'lambda_l2': 1.2892629852592699, 'bagging_freq': 3, 'min_child_samples': 39}. Best is trial 5 with value: 0.051326789555165744.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'objective': 'multiclass',\n",
       " 'num_class': 10,\n",
       " 'learning_rate': 0.06435218111142486,\n",
       " 'lambda_l1': 1.433532882656931,\n",
       " 'lambda_l2': 9.44668917104915,\n",
       " 'bagging_freq': 4,\n",
       " 'min_child_samples': 44,\n",
       " 'max_depth': -1,\n",
       " 'random_state': 0,\n",
       " 'num_boost_round': 1000,\n",
       " 'verbosity': -100}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### lightgbm\n",
    "### 深さが浅い・中・深いものを作る\n",
    "\n",
    "# 浅いやつ\n",
    "lgbm = models.Lgbm({})\n",
    "optimized_params = beyesian_optimization(lgbm, X_train, y_train, {\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': 10,\n",
    "    'learning_rate': [0.001, 0.1],\n",
    "    'lambda_l1': [1e-8, 10.0],\n",
    "    'lambda_l2': [1e-8, 10.0],\n",
    "    'bagging_freq': [1, 7],\n",
    "    'min_child_samples': [5, 100],\n",
    "    'learning_rate': [0.001, 0.1],\n",
    "    'max_depth': -1,\n",
    "    'random_state': 0,\n",
    "    'num_boost_round': 1000,\n",
    "    'verbosity': -100,\n",
    "}, n_trials)\n",
    "optimized_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'objective': 'multiclass', 'num_class': 10, 'learning_rate': 0.06435218111142486, 'lambda_l1': 1.433532882656931, 'lambda_l2': 9.44668917104915, 'bagging_freq': 4, 'min_child_samples': 44, 'max_depth': -1, 'random_state': 0, 'num_boost_round': 1000, 'verbosity': -100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yutahirai/opt/anaconda3/envs/tensorflow26/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/yutahirai/opt/anaconda3/envs/tensorflow26/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/Users/yutahirai/opt/anaconda3/envs/tensorflow26/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/yutahirai/opt/anaconda3/envs/tensorflow26/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/Users/yutahirai/opt/anaconda3/envs/tensorflow26/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/yutahirai/opt/anaconda3/envs/tensorflow26/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/Users/yutahirai/opt/anaconda3/envs/tensorflow26/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/yutahirai/opt/anaconda3/envs/tensorflow26/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/Users/yutahirai/opt/anaconda3/envs/tensorflow26/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/yutahirai/opt/anaconda3/envs/tensorflow26/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98868\n"
     ]
    }
   ],
   "source": [
    "print(optimized_params)\n",
    "print(cross_validation_score(models.Lgbm(optimized_params), X_train, y_train))\n",
    "with open('./config/Lgbm-depth-inf.json', 'w') as f:\n",
    "    json.dump(optimized_params, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-02-23 13:00:35,534]\u001b[0m A new study created in memory with name: no-name-98d07916-a60b-4b02-b610-3026a5a58c41\u001b[0m\n",
      "\u001b[32m[I 2022-02-23 13:06:12,266]\u001b[0m Trial 0 finished with value: 0.435211419117624 and parameters: {'n_estimators': 562, 'max_depth': 184, 'min_samples_split': 39, 'min_samples_leaf': 18}. Best is trial 0 with value: 0.435211419117624.\u001b[0m\n",
      "\u001b[32m[I 2022-02-23 13:10:21,408]\u001b[0m Trial 1 finished with value: 0.5090524798944512 and parameters: {'n_estimators': 434, 'max_depth': 166, 'min_samples_split': 29, 'min_samples_leaf': 29}. Best is trial 0 with value: 0.435211419117624.\u001b[0m\n",
      "\u001b[32m[I 2022-02-23 13:20:06,412]\u001b[0m Trial 2 finished with value: 0.45100757525284785 and parameters: {'n_estimators': 987, 'max_depth': 99, 'min_samples_split': 51, 'min_samples_leaf': 17}. Best is trial 0 with value: 0.435211419117624.\u001b[0m\n",
      "\u001b[32m[I 2022-02-23 13:27:04,174]\u001b[0m Trial 3 finished with value: 0.17847137623155981 and parameters: {'n_estimators': 582, 'max_depth': 237, 'min_samples_split': 6, 'min_samples_leaf': 3}. Best is trial 3 with value: 0.17847137623155981.\u001b[0m\n",
      "\u001b[32m[I 2022-02-23 13:27:17,338]\u001b[0m Trial 4 finished with value: 0.5097223859882465 and parameters: {'n_estimators': 21, 'max_depth': 214, 'min_samples_split': 51, 'min_samples_leaf': 28}. Best is trial 3 with value: 0.17847137623155981.\u001b[0m\n",
      "\u001b[32m[I 2022-02-23 13:37:23,880]\u001b[0m Trial 5 finished with value: 0.48509335595158265 and parameters: {'n_estimators': 1003, 'max_depth': 205, 'min_samples_split': 31, 'min_samples_leaf': 25}. Best is trial 3 with value: 0.17847137623155981.\u001b[0m\n",
      "\u001b[32m[I 2022-02-23 13:38:46,482]\u001b[0m Trial 6 finished with value: 0.5188278837259042 and parameters: {'n_estimators': 122, 'max_depth': 164, 'min_samples_split': 11, 'min_samples_leaf': 31}. Best is trial 3 with value: 0.17847137623155981.\u001b[0m\n",
      "\u001b[32m[I 2022-02-23 13:44:00,701]\u001b[0m Trial 7 finished with value: 0.48548159159094995 and parameters: {'n_estimators': 535, 'max_depth': 107, 'min_samples_split': 18, 'min_samples_leaf': 25}. Best is trial 3 with value: 0.17847137623155981.\u001b[0m\n",
      "\u001b[32m[I 2022-02-23 13:49:05,682]\u001b[0m Trial 8 finished with value: 0.4489359440722332 and parameters: {'n_estimators': 468, 'max_depth': 146, 'min_samples_split': 3, 'min_samples_leaf': 20}. Best is trial 3 with value: 0.17847137623155981.\u001b[0m\n",
      "\u001b[32m[I 2022-02-23 13:56:00,820]\u001b[0m Trial 9 finished with value: 0.48694255283389687 and parameters: {'n_estimators': 627, 'max_depth': 158, 'min_samples_split': 61, 'min_samples_leaf': 22}. Best is trial 3 with value: 0.17847137623155981.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "rf = models.RandomForest({})\n",
    "optimized_params = beyesian_optimization(rf, X_train, y_train, {\n",
    "    'n_estimators': [1, 1024],\n",
    "    'max_depth': [1, 256],\n",
    "    'min_samples_split': [2, 64],\n",
    "    'min_samples_leaf': [1, 32],\n",
    "    'random_state': 0\n",
    "}, n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 582, 'max_depth': 237, 'min_samples_split': 6, 'min_samples_leaf': 3, 'random_state': 0}\n",
      "0.99416\n"
     ]
    }
   ],
   "source": [
    "print(optimized_params)\n",
    "print(cross_validation_score(models.RandomForest(optimized_params), X_train, y_train))\n",
    "with open('./config/RandomForest.json', 'w') as f:\n",
    "    json.dump(optimized_params, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-02-23 14:41:20,496]\u001b[0m A new study created in memory with name: no-name-1f1aad45-9db3-4c28-a0b0-2b16b5346f17\u001b[0m\n",
      "\u001b[32m[I 2022-02-23 14:44:10,421]\u001b[0m Trial 0 finished with value: 0.4561213837452121 and parameters: {'n_estimators': 563, 'max_depth': 184, 'min_samples_split': 39, 'min_samples_leaf': 18}. Best is trial 0 with value: 0.4561213837452121.\u001b[0m\n",
      "\u001b[32m[I 2022-02-23 14:46:15,603]\u001b[0m Trial 1 finished with value: 0.5584812529072248 and parameters: {'n_estimators': 435, 'max_depth': 166, 'min_samples_split': 29, 'min_samples_leaf': 29}. Best is trial 0 with value: 0.4561213837452121.\u001b[0m\n",
      "\u001b[32m[I 2022-02-23 14:51:08,048]\u001b[0m Trial 2 finished with value: 0.466995486925935 and parameters: {'n_estimators': 987, 'max_depth': 99, 'min_samples_split': 51, 'min_samples_leaf': 17}. Best is trial 0 with value: 0.4561213837452121.\u001b[0m\n",
      "\u001b[32m[I 2022-02-23 14:54:35,034]\u001b[0m Trial 3 finished with value: 0.13875562849652004 and parameters: {'n_estimators': 583, 'max_depth': 237, 'min_samples_split': 6, 'min_samples_leaf': 3}. Best is trial 3 with value: 0.13875562849652004.\u001b[0m\n",
      "\u001b[32m[I 2022-02-23 14:54:42,377]\u001b[0m Trial 4 finished with value: 0.5485648812916736 and parameters: {'n_estimators': 22, 'max_depth': 214, 'min_samples_split': 51, 'min_samples_leaf': 28}. Best is trial 3 with value: 0.13875562849652004.\u001b[0m\n",
      "\u001b[32m[I 2022-02-23 14:59:52,605]\u001b[0m Trial 5 finished with value: 0.525789275227751 and parameters: {'n_estimators': 1003, 'max_depth': 205, 'min_samples_split': 31, 'min_samples_leaf': 25}. Best is trial 3 with value: 0.13875562849652004.\u001b[0m\n",
      "\u001b[32m[I 2022-02-23 15:00:29,105]\u001b[0m Trial 6 finished with value: 0.5751040256617495 and parameters: {'n_estimators': 122, 'max_depth': 164, 'min_samples_split': 11, 'min_samples_leaf': 31}. Best is trial 3 with value: 0.13875562849652004.\u001b[0m\n",
      "\u001b[32m[I 2022-02-23 15:03:06,738]\u001b[0m Trial 7 finished with value: 0.5254636706619539 and parameters: {'n_estimators': 535, 'max_depth': 107, 'min_samples_split': 18, 'min_samples_leaf': 25}. Best is trial 3 with value: 0.13875562849652004.\u001b[0m\n",
      "\u001b[32m[I 2022-02-23 15:05:22,068]\u001b[0m Trial 8 finished with value: 0.4770221336981857 and parameters: {'n_estimators': 468, 'max_depth': 146, 'min_samples_split': 3, 'min_samples_leaf': 20}. Best is trial 3 with value: 0.13875562849652004.\u001b[0m\n",
      "\u001b[32m[I 2022-02-23 15:08:19,058]\u001b[0m Trial 9 finished with value: 0.5153728383172482 and parameters: {'n_estimators': 628, 'max_depth': 158, 'min_samples_split': 61, 'min_samples_leaf': 22}. Best is trial 3 with value: 0.13875562849652004.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "etr = models.ERT({})\n",
    "optimized_params = beyesian_optimization(etr, X_train, y_train, {\n",
    "    'n_estimators': [2, 1024],\n",
    "    'max_depth': [1, 256],\n",
    "    'min_samples_split': [2, 64],\n",
    "    'min_samples_leaf': [1, 32],\n",
    "    'random_state': 0\n",
    "}, n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 583, 'max_depth': 237, 'min_samples_split': 6, 'min_samples_leaf': 3, 'random_state': 0}\n",
      "0.994935\n"
     ]
    }
   ],
   "source": [
    "print(optimized_params)\n",
    "print(cross_validation_score(models.ERT(optimized_params), X_train, y_train))\n",
    "with open('./config/ERT.json', 'w') as f:\n",
    "    json.dump(optimized_params, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn = models.NN({})\n",
    "# optimized_params = beyesian_optimization(nn, X_train, y_train, {\n",
    "#     'layers': 4,\n",
    "#     'dropout': [0.00001, 0.9],\n",
    "#     'units': 10,\n",
    "#     'nb_epoch': 100,\n",
    "# }, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(optimized_params)\n",
    "# nn = models.NN(optimized_params)\n",
    "# print(cross_validation_score(nn, X_train, y_train))\n",
    "# with open('./config/NN-shallow.json', 'w') as f:\n",
    "#     json.dump(optimized_params, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn = models.NN({})\n",
    "# optimized_params = beyesian_optimization(nn, X_train, y_train, {\n",
    "#     'layers': 10,\n",
    "#     'dropout': [0.00001, 0.9],\n",
    "#     'units': 20,\n",
    "#     'nb_epoch': 100\n",
    "# }, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(optimized_params)\n",
    "# nn = models.NN(optimized_params)\n",
    "# # nn.save_model('./config/NN-deep.h5')\n",
    "# print(cross_validation_score(nn, X_train, y_train))\n",
    "# with open('./config/NN-deep.json', 'w') as f:\n",
    "#     json.dump(optimized_params, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-23 15:29:01.781343: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-23 15:29:01.799905: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-02-23 15:29:01.801607: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2022-02-23 15:29:04.513763: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-23 15:50:38.952263: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-23 15:50:49.147997: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-23 16:16:01.773985: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-23 16:16:10.504382: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-23 16:41:55.743899: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-23 16:42:08.109777: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-23 17:09:01.012560: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-23 17:09:25.793760: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "# とりあえず決め打ち\n",
    "optimized_params = {\n",
    "    'patience': 30,\n",
    "    'layers': 2,\n",
    "    'dropout': 0.1,\n",
    "    'units': 100,\n",
    "    'nb_epoch': 100,\n",
    "    'batch_size': 256\n",
    "}\n",
    "nn = models.NN(optimized_params)\n",
    "print(cross_validation_score(nn, X_train, y_train))\n",
    "print(optimized_params)\n",
    "with open('./config/NN-shallow.json', 'w') as f:\n",
    "    json.dump(optimized_params, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# とりあえず決め打ち\n",
    "optimized_params = {\n",
    "    'patience': 30,\n",
    "    'layers': 8,\n",
    "    'dropout': 0.1,\n",
    "    'units': 100,\n",
    "    'nb_epoch': 1000,\n",
    "    'batch_size': 256\n",
    "}\n",
    "nn = models.NN(optimized_params)\n",
    "print(cross_validation_score(nn, X_train, y_train))\n",
    "print(optimized_params)\n",
    "with open('./config/NN-deep.json', 'w') as f:\n",
    "    json.dump(optimized_params, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3f91d3689ed9f35fe62b472acb20385518ac1a4462c3f395fef40efd7e4015f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('matlab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
